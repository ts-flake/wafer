# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_layers.ipynb.

# %% auto 0
__all__ = ['lambda_init', 'lsuv_init', 'default_init', 'rai_init']

# %% ../nbs/00_layers.ipynb 3
from fastcore.all import *
from fastai.torch_basics import *
from fastai.callback.hook import Hook

# %% ../nbs/00_layers.ipynb 5
_fc_conv_filter = (nn.Linear, nn.Conv1d, nn.Conv2d, nn.Conv3d,
                   nn.ConvTranspose1d, nn.ConvTranspose2d, nn.ConvTranspose3d)

def lambda_init(m: nn.Module, func: callable=lambda w,b: (nn.init.kaiming_normal_(w), nn.init.zeros_(b))):
    "Initialize the `weight` and `bias` of a model `m` with `func`."
    for l in m.modules():
        if isinstance(l, _fc_conv_filter):
            if l.bias is None: func(l.weight, torch.empty(1))
            else:              func(l.weight, l.bias)

# %% ../nbs/00_layers.ipynb 6
def lsuv_init(m: nn.Module, # model
              xb: torch.Tensor, # mini-batch input
              tol: float=0.01, # tolerance
              n_iter: int=10, # maximum number of iterations
              verbose: bool=False, # print out details
             ):
    "Refer to [All you need is a good init](https://arxiv.org/abs/1511.06422)."
    xb = xb.cpu()
    m.to(xb.device)

    # orthogonal init
    lambda_init(m, lambda w,b: (nn.init.orthogonal_(w), nn.init.zeros_(b)))

    # LSUV init
    m.eval()
    for l in m.modules():
        if isinstance(l, _fc_conv_filter):
            n = 0
            h = Hook(l, lambda m,i,o: (o.mean().item(), o.std().item()), cpu=True)
            with torch.inference_mode():
                while (m(xb) is not None
                       and ((l.bias is not None and abs(h.stored[0] - 0.) > tol) or abs(h.stored[1] - 1.) > tol)
                       and n < n_iter):
                    l.weight /= (h.stored[1] + 1e-8)
                    if l.bias is not None: l.bias -= h.stored[0]
                    n += 1
            if verbose: print(f"{str(l):80}| took {n} iterations, mean={h.stored[0]:7.4f}, std={h.stored[1]:.4f}")
            h.remove()

# %% ../nbs/00_layers.ipynb 13
def default_init(m: nn.Module, # model
                 normal: bool=True, # use normal distribution
                 verbose: bool=False, # print out details
                ):
    "Initialize weights of `nn.Linear` and `nn.ConvXd` using Xavier's or Kaiming's method; zero biases; custom gains."
    if normal: xavier = nn.init.xavier_normal_;  kaiming = nn.init.kaiming_normal_
    else:      xavier = nn.init.xavier_uniform_; kaiming = nn.init.kaiming_uniform_
    _txt = 'normal' if normal else 'uniform'
    
    _actn_filter = (nn.Tanh, nn.Sigmoid, nn.Softplus, nn.Softsign,
                    nn.ReLU, nn.LeakyReLU, nn.SiLU, nn.GELU, nn.ELU)
    _m = list(filter(lambda o: isinstance(o, _fc_conv_filter + _actn_filter), m.modules()))
    
    for l,lm in zip(_m, _m[0:1]+_m[:-1]): # l-th and (l-1)-th layers
        if isinstance(l, _fc_conv_filter):
            # ReLU and its variants
            if isinstance(lm, (nn.ReLU, nn.LeakyReLU, nn.SiLU, nn.GELU, nn.Softsign)):
                gain = lm.negative_slope if isinstance(lm, nn.LeakyReLU) else 0.
                kaiming(l.weight, gain, nonlinearity='leaky_relu')
                if verbose: print(f"{str(l):80}| He_{_txt}, negative_slope={gain}")
            
            # Custom gains by trial and error
            elif isinstance(lm, (nn.Tanh, nn.Sigmoid, nn.Softplus, nn.ELU)):
                gain = 1.79 if isinstance(lm, nn.Sigmoid) else 1.17
                xavier(l.weight, gain)
                if verbose: print(f"{str(l):80}| Xavier_{_txt}, gain={gain}")
            else:
                xavier(l.weight)
                if verbose: print(f"{str(l):80}| Xavier_{_txt}, gain=1.")
            if l.bias is not None: nn.init.zeros_(l.bias)

# %% ../nbs/00_layers.ipynb 16
def rai_init(m: nn.Module):
    "Randomized asymmetric initializer. \
    Refer to [Dying ReLU and Initialization: Theory and Numerical Examples](https://arxiv.org/abs/1903.06733)"
    _is_first = True
    
    for l in m.modules():
        if isinstance(l, _fc_conv_filter):
            if _is_first:
                nn.init.kaiming_normal_(l.weight)
                if l.bias is not None: nn.init.zeros_(l.bias)
                _is_first = False; continue
            # Get correct fan_in and fan_out
            if l.weight.ndim == 2:
                fan_out,fan_in = l.weight.shape
            else:
                fan_out = l.weight.shape[0]
                fan_in = np.prod(l.weight.shape[1:])
            # RAI
            V = torch.randn(fan_out, fan_in+1) * 0.6007 / (fan_in ** 0.5)
            for j in range(fan_out):
                k = np.random.randint(0, fan_in+1)
                V[j,k] = np.random.beta(2, 1)
            with torch.no_grad():
                l.weight.copy_(V[:, :-1].reshape(l.weight.shape))
                if l.bias is not None: l.bias.copy_(V[:, -1])
