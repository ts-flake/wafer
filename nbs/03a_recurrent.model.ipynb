{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4996a952-93a0-45f8-a8e6-99fb7eb71c95",
   "metadata": {},
   "source": [
    "# RNN models\n",
    "> Various RNNs and dynamical models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cbe657-df09-44f3-add8-feed259c35f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp recurrent.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4afdf76-362c-4f67-898a-bcfebb39f420",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fce64b-38d7-43c0-9bb1-19cb1f3903cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from wafer.basics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9288051e-5fee-463d-af8c-09f33767e43a",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8636df15-75ff-4883-ba9d-18d123ba25dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Select(nn.Module):\n",
    "    \"Select from iterable.\"\n",
    "    def __init__(self, idx=0):\n",
    "        super().__init__()\n",
    "        self.idx = idx\n",
    "    def forward(self, x):\n",
    "        return x[self.idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7680d7-c5d8-432e-bcc3-933cda787639",
   "metadata": {},
   "source": [
    "## Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de3fa7e-4d3c-4847-a721-92b16f9f5368",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from wafer.init import default_init, lambda_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb941ee-ac71-43ee-94f6-335b1fcb1be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SimpleRNN(nn.Module):\n",
    "    \"A RNN with its output mapped through a dense layer.\"\n",
    "    def __init__(self, ni, nh, no, num_layers=2,\n",
    "                 actn='tanh',   # nonlinearity, ['tanh', 'relu']\n",
    "                 init='normal', # initialization method ['uniform', 'normal', 'irnn', 'np-rnn']\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.recurrent = nn.RNN(ni, nh, num_layers=num_layers, batch_first=True, nonlinearity=actn)\n",
    "        self.dense = nn.Linear(nh, no)\n",
    "        # Initialize\n",
    "        self._init(self.recurrent, nh, init)\n",
    "        default_init(self.dense)\n",
    "\n",
    "    @staticmethod\n",
    "    @torch.no_grad()\n",
    "    def _init(m, nh, init):\n",
    "        for n,p in m.named_parameters():\n",
    "            if 'bias' in n:\n",
    "                nn.init.zeros_(p)\n",
    "            if 'weight_ih' in n:\n",
    "                if init == 'uniform':\n",
    "                    nn.init.uniform_(p, a=-1/np.sqrt(nh), b=1/np.sqrt(nh))\n",
    "                elif init == 'np-rnn':\n",
    "                    nn.init.normal_(p, std=1/np.sqrt(nh))\n",
    "                    p = p * (np.sqrt(2) * np.exp(1.2 / (max(nh, 6) - 2.4)))\n",
    "                    getattr(m, n).copy_(p)\n",
    "                else:\n",
    "                    nn.init.normal_(p, std=1/np.sqrt(nh))\n",
    "            if 'weight_hh' in n:\n",
    "                if init == 'uniform':\n",
    "                    nn.init.uniform_(p, a=-1/np.sqrt(nh), b=1/np.sqrt(nh))\n",
    "                elif init == 'irnn':\n",
    "                    nn.init.eye_(p)\n",
    "                elif init == 'np-rnn':\n",
    "                    nn.init.normal_(p)\n",
    "                    p = (p.T @ p) / nh\n",
    "                    p = p / np.linalg.eigvals(p.detach().numpy()).max()\n",
    "                    getattr(m, n).copy_(p)\n",
    "                else:\n",
    "                    nn.init.normal_(p, std=1/np.sqrt(nh))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        outp,_ = self.recurrent(x) # shape(N, L, nh)\n",
    "        return self.dense(outp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbbdda8-9d1f-4bdf-81f8-11ffd8d53680",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LSTM(nn.Module):\n",
    "    \"\"\"Custom LSTM, allowing for different activations. Assuming `batch_first=True` and `bidirectional=False`.\n",
    "    \n",
    "    Inputs: x, h0_c0;\n",
    "        x: shape (N, L, input_size)\n",
    "        h0_c0: optional, default zeros, shape (num_layers, N, hidden_size)\n",
    "    \n",
    "    Outputs: output, (hn, cn);\n",
    "        output: shape (N, L, hidden_size), outputs of the last layer for each token.\n",
    "        hn: shape (N, num_layers, hidden_size), final hidden state.\n",
    "        cn: shape (N, num_layers, hidden_size), final cell state.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, ni, nh, num_layers=1, actn='tanh', gate_actn='sigmoid',\n",
    "                 dropout=0.0, unit_forget_bias=True, init_gain=1/np.sqrt(3), recurrent_init_gain=1.):\n",
    "        super().__init__()\n",
    "        self.nh,self.num_layers = nh,num_layers\n",
    "        self.actn = getattr(F, actn, 'tanh')\n",
    "        self.gate_actn = getattr(F, gate_actn, 'sigmoid')\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        ws = []\n",
    "        for i in range(num_layers):\n",
    "            ws.append(nn.ModuleDict({\n",
    "                'ii': nn.Linear(ni if i == 0 else nh, nh),  # input gate\n",
    "                'hi': nn.Linear(nh, nh),\n",
    "                'if': nn.Linear(ni if i == 0 else nh, nh),  # forget gate\n",
    "                'hf': nn.Linear(nh, nh),\n",
    "                'io': nn.Linear(ni if i == 0 else nh, nh),  # output gate\n",
    "                'ho': nn.Linear(nh, nh),\n",
    "                'ic': nn.Linear(ni if i == 0 else nh, nh),  # cell gate\n",
    "                'hc': nn.Linear(nh, nh)\n",
    "            }))\n",
    "        self.ws = nn.ModuleList(ws)\n",
    "        \n",
    "        # Initialize\n",
    "        for n,p in self.named_parameters():\n",
    "            # hidden/recurrent weight\n",
    "            if '.h' in n:\n",
    "                if 'bias' in n:\n",
    "                    nn.init.zeros_(p)\n",
    "                else:\n",
    "                    nn.init.orthogonal_(p, recurrent_init_gain)\n",
    "            # non-recurrent weight\n",
    "            else:\n",
    "                if 'bias' in n:\n",
    "                    if '.if' in n and unit_forget_bias:\n",
    "                        nn.init.ones_(p)\n",
    "                    else:\n",
    "                        nn.init.zeros_(p)\n",
    "                else:\n",
    "                    nn.init.xavier_uniform_(p, init_gain) \n",
    "    \n",
    "    def _forward_single(self, x, h0_c0: list=None):\n",
    "        \"Forward pass of a single token.\"\n",
    "        if h0_c0 is None:\n",
    "            h0 = torch.zeros(self.num_layers, x.shape[0], self.nh, device=x.device)\n",
    "            c0 = torch.zeros(self.num_layers, x.shape[0], self.nh, device=x.device)\n",
    "        else:\n",
    "            h0,c0 = h0_c0\n",
    "            assert (h0.shape[-1] == c0.shape[-1] == self.nh) and (h0.shape[1] == c0.shape[1] == x.shape[0])\n",
    "        \n",
    "        hs, cs = [], []\n",
    "        for i in range(self.num_layers):\n",
    "            h, c = h0[i], c0[i]\n",
    "            i_gate = self.ws[i]['ii'](x) + self.ws[i]['hi'](h)\n",
    "            f_gate = self.ws[i]['if'](x) + self.ws[i]['hf'](h)\n",
    "            o_gate = self.ws[i]['io'](x) + self.ws[i]['ho'](h)\n",
    "            c_gate = self.ws[i]['ic'](x) + self.ws[i]['hc'](h)\n",
    "\n",
    "            i_gate = self.gate_actn(i_gate)\n",
    "            f_gate = self.gate_actn(f_gate)\n",
    "            o_gate = self.gate_actn(o_gate)\n",
    "            c_gate = self.actn(c_gate)\n",
    "\n",
    "            c_new = (f_gate * c) + (i_gate * c_gate)\n",
    "            h_new = o_gate * self.actn(c_new)\n",
    "            cs.append(c_new)\n",
    "            hs.append(h_new)\n",
    "            x = self.dropout(h_new)\n",
    "\n",
    "        hs, cs = torch.stack(hs, 0), torch.stack(cs, 0)\n",
    "        return (hs, cs)\n",
    "\n",
    "    def forward(self, x, h0_c0=None):\n",
    "        # Input `x`, shape (N,L,D); `h0_c0`, shape (num_layers, N, nh).\n",
    "        hs = []\n",
    "        cs = []\n",
    "        for xi in torch.permute(x, [1,0,2]):\n",
    "            hc = self._forward_single(xi, h0_c0)\n",
    "            hs.append(hc[0])\n",
    "            cs.append(hc[1])\n",
    "            h0_c0 = hc\n",
    "        output = torch.stack([h[-1] for h in hs], 1)\n",
    "        return output, (hs[-1], cs[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f058e9e-de50-4852-bea0-e885f95eb1c8",
   "metadata": {},
   "source": [
    "## Input convex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a00c73-cbdb-4833-a963-de8dd8a3e868",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FICNN(nn.Module):\n",
    "    \"Fully input-convex NN. Refer to [ICNN](https://arxiv.org/abs/1609.07152).\"\n",
    "    def __init__(self, ni: int,              # input size\n",
    "                 nh: Union[int, list[int]],  # hidden size\n",
    "                 no: int,                    # output size\n",
    "                 num_layer: int=2,           # number of layers (include the output layer), if `nh` is a list, then the `num_layer = len(nh) + 1`\n",
    "                 actn: str='relu',           # hidden activation\n",
    "                 out_actn: str=None,         # output activation\n",
    "                 init_gain: float=1.         # weight initialization gain\n",
    "                ):\n",
    "        super().__init__()\n",
    "        nhs = [nh] * (num_layer - 1) if isinstance(nh, int) else nh\n",
    "        self.w_y = nn.ModuleList([nn.Linear(ni, nh) for nh in nhs + [no]])\n",
    "        self.w_z = nn.ModuleList([nn.Linear(i, j, bias=False) for i,j in zip(nhs, nhs[1:] + [no])])\n",
    "        self.actn = getattr(F, actn)\n",
    "        self.out_actn = getattr(F, out_actn) if out_actn is not None else noop\n",
    "\n",
    "        lambda_init(self, lambda w,b: (nn.init.xavier_normal_(w, init_gain), nn.init.zeros_(b)))\n",
    "        self.weight_constraint()\n",
    "    \n",
    "    def weight_constraint(self):\n",
    "        \"Apply nonnegative weight constriant.\"\n",
    "        with torch.no_grad():\n",
    "            for n,p in self.w_z.named_parameters():\n",
    "                if 'weight' in n:\n",
    "                    p.clamp_min_(0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.w_y[0](x)\n",
    "        if len(self.w_z) == 0: return self.out_actn(z)\n",
    "        for wz,wy in zip(self.w_z[:-1], self.w_y[1:-1]):\n",
    "            z = self.actn(wz(z) + wy(x))\n",
    "        z = self.out_actn(self.w_z[-1](z) + self.w_y[-1](x))\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545f5b4a-7f52-4d72-a945-485f669c2d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ICRNN(nn.Module):\n",
    "    \"\"\"Input convex RNN. Refer to [Optimal Control Via Neural Networks: A Convex Approach](https://arxiv.org/abs/1805.11835).\n",
    "    \n",
    "    Inputs:  x, shape (N,L,D_in)\n",
    "    Outputs: outputs, shape (N,L,D_out)\n",
    "    \"\"\"\n",
    "    def __init__(self, ni: int,              # input size\n",
    "                 nh: int,                    # hidden size\n",
    "                 no: int,                    # output size\n",
    "                 actn: str='relu',           # hidden activation\n",
    "                 out_actn: str=None,         # output activation\n",
    "                 expand_inp: bool=True,      # expand the input to [x, -x]\n",
    "                 init_gain: float=1.         # weight initialization gain\n",
    "                ):\n",
    "        super().__init__()\n",
    "        ni = ni * 2 if expand_inp else ni\n",
    "        self.expand_inp,self.nh = expand_inp,nh\n",
    "        self.actn = getattr(F, actn)\n",
    "        self.out_actn = getattr(F, out_actn) if out_actn is not None else noop\n",
    "        self.U = nn.Linear(ni, nh)\n",
    "        self.V = nn.Linear(nh, no)\n",
    "        self.W = nn.Linear(nh, nh)\n",
    "        self.D1 = nn.Linear(nh, no)\n",
    "        self.D2 = nn.Linear(ni, nh)\n",
    "        self.D3 = nn.Linear(ni, no)\n",
    "\n",
    "        lambda_init(self, lambda w,b: (nn.init.xavier_normal_(w, init_gain), nn.init.zeros_(b)))\n",
    "        self.weight_constraint()\n",
    "\n",
    "    def weight_constraint(self):\n",
    "        \"Apply nonnegative weight constriant.\"\n",
    "        with torch.no_grad():\n",
    "            for n,p in self.named_parameters():\n",
    "                if 'weight' in n:\n",
    "                    p.clamp_min_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.expand_inp: x = torch.cat([x, -x], dim=-1)\n",
    "        x = torch.cat([torch.zeros(x.shape[0],1,x.shape[-1], device=x.device), x], dim=-2)\n",
    "        z_curr = z_prev = torch.zeros(x.shape[0], self.nh, device=x.device)\n",
    "        ys = []\n",
    "        for xi,xj in zip(x[:,:-1,:].permute(1,0,2), x[:,1:,:].permute(1,0,2)):\n",
    "            z_curr = self.actn(self.U(xj) + self.W(z_prev) + self.D2(xi))\n",
    "            ys.append(self.out_actn(self.V(z_curr) + self.D1(z_prev) + self.D3(xj)))\n",
    "            z_prev = z_curr\n",
    "        return torch.stack(ys, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0427aca-6ae0-478e-bfa2-a86c45827bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ICLSTM(nn.Module):\n",
    "    \"\"\"Input convex LSTM. From [ICLSTM](https://arxiv.org/abs/2311.07202).\n",
    "    A L-ICLSTM is a stack of [LSTM, Linear, ReLU] repeated L times. The output is mapped through another Linear layer.\n",
    "    For it to be input convex, all weights are constrained to be non-negative, \n",
    "    and all activations are convex, non-decreasing and non-negative (no activation is applied to the output in the code.)\n",
    "    \"\"\"\n",
    "    def __init__(self, ni, nh, no, num_layers=2, expand_inp=True, actn='relu', gate_actn='relu', **kwargs):\n",
    "        super().__init__()\n",
    "        ni = ni * 2 if expand_inp else ni\n",
    "        self.expand_inp = expand_inp\n",
    "        self.actn = getattr(F, actn, 'relu')\n",
    "        self.blocks = nn.ModuleList([self._mk_block(ni, nh, actn, gate_actn, **kwargs) for _ in range(num_layers)])\n",
    "        self.dense_out = nn.Linear(ni, no)\n",
    "        # Nonnegative weights\n",
    "        self.weight_constraint()\n",
    "\n",
    "    def _mk_block(self, ni, nh, actn, gate_actn, **kwargs):\n",
    "        m = nn.Sequential(LSTM(ni, nh, 1, actn=actn, gate_actn=gate_actn, **kwargs),\n",
    "                          Select(), nn.Linear(nh, ni))\n",
    "        default_init(m[2], normal=False)\n",
    "        return m\n",
    "        \n",
    "    def weight_constraint(self):\n",
    "        \"Apply nonnegative weight constriant.\"\n",
    "        with torch.no_grad():\n",
    "            for n,p in self.named_parameters():\n",
    "                if 'weight' in n:\n",
    "                    p.clamp_min_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.expand_inp: x = torch.cat([x, -x], dim=-1)\n",
    "        x0 = x.clone()\n",
    "        for b in self.blocks:\n",
    "            x = self.actn(b(x)) + x0\n",
    "        return self.dense_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dd444c-d8ce-4228-9df6-1bfe8b8fe459",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class WeightConstraintCB(Callback):\n",
    "    \"Weight constraint callback.\"\n",
    "    order = 10\n",
    "    def after_step(self): self.learner.model.weight_constraint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2f154a-b01e-4286-b768-513377fc87f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd9b96c-7a2b-4b4a-8570-50a8701b97b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 61 column 1 (char 1196)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#| hide\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnbdev\u001b[39;00m; nbdev\u001b[38;5;241m.\u001b[39mnbdev_export()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/fastcore/script.py:110\u001b[0m, in \u001b[0;36mcall_parse.<locals>._f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    109\u001b[0m     mod \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetmodule(inspect\u001b[38;5;241m.\u001b[39mcurrentframe()\u001b[38;5;241m.\u001b[39mf_back)\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mod: \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SCRIPT_INFO\u001b[38;5;241m.\u001b[39mfunc \u001b[38;5;129;01mand\u001b[39;00m mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m: SCRIPT_INFO\u001b[38;5;241m.\u001b[39mfunc \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sys\u001b[38;5;241m.\u001b[39margv)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m sys\u001b[38;5;241m.\u001b[39margv[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m: sys\u001b[38;5;241m.\u001b[39margv\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/nbdev/doclinks.py:142\u001b[0m, in \u001b[0;36mnbdev_export\u001b[0;34m(path, procs, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m   procs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mgetattr\u001b[39m(nbdev\u001b[38;5;241m.\u001b[39mexport, p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m L(procs)]\n\u001b[1;32m    141\u001b[0m files \u001b[38;5;241m=\u001b[39m nbglob(path\u001b[38;5;241m=\u001b[39mpath, as_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39msorted(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files: nb_export(f, procs\u001b[38;5;241m=\u001b[39mprocs)\n\u001b[1;32m    143\u001b[0m add_init(get_config()\u001b[38;5;241m.\u001b[39mlib_path)\n\u001b[1;32m    144\u001b[0m _build_modidx()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/nbdev/export.py:67\u001b[0m, in \u001b[0;36mnb_export\u001b[0;34m(nbname, lib_path, procs, debug, mod_maker, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lib_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: lib_path \u001b[38;5;241m=\u001b[39m get_config()\u001b[38;5;241m.\u001b[39mlib_path\n\u001b[1;32m     66\u001b[0m exp \u001b[38;5;241m=\u001b[39m ExportModuleProc()\n\u001b[0;32m---> 67\u001b[0m nb \u001b[38;5;241m=\u001b[39m NBProcessor(nbname, [exp]\u001b[38;5;241m+\u001b[39mL(procs), debug\u001b[38;5;241m=\u001b[39mdebug)\n\u001b[1;32m     68\u001b[0m nb\u001b[38;5;241m.\u001b[39mprocess()\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod,cells \u001b[38;5;129;01min\u001b[39;00m exp\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/nbdev/process.py:93\u001b[0m, in \u001b[0;36mNBProcessor.__init__\u001b[0;34m(self, path, procs, nb, debug, rm_directives, process)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, procs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, nb\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, rm_directives\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, process\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb \u001b[38;5;241m=\u001b[39m read_nb(path) \u001b[38;5;28;01mif\u001b[39;00m nb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m nb\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlang \u001b[38;5;241m=\u001b[39m nb_lang(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb)\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cell \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb\u001b[38;5;241m.\u001b[39mcells: cell\u001b[38;5;241m.\u001b[39mdirectives_ \u001b[38;5;241m=\u001b[39m extract_directives(cell, remove\u001b[38;5;241m=\u001b[39mrm_directives, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlang)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/execnb/nbio.py:57\u001b[0m, in \u001b[0;36mread_nb\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_nb\u001b[39m(path):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturn notebook at `path`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 57\u001b[0m     res \u001b[38;5;241m=\u001b[39m dict2nb(_read_json(path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     58\u001b[0m     res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath_\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(path)\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/execnb/nbio.py:16\u001b[0m, in \u001b[0;36m_read_json\u001b[0;34m(self, encoding, errors)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(Path(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mread_text(encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_decoder\u001b[38;5;241m.\u001b[39mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_decode(s, idx\u001b[38;5;241m=\u001b[39m_w(s, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mend())\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 61 column 1 (char 1196)"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
