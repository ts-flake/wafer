{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4996a952-93a0-45f8-a8e6-99fb7eb71c95",
   "metadata": {},
   "source": [
    "# RNN models\n",
    "> Various RNNs and dynamical models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cbe657-df09-44f3-add8-feed259c35f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp recurrent.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4afdf76-362c-4f67-898a-bcfebb39f420",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fce64b-38d7-43c0-9bb1-19cb1f3903cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from wafer.basics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9288051e-5fee-463d-af8c-09f33767e43a",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8636df15-75ff-4883-ba9d-18d123ba25dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Select(nn.Module):\n",
    "    \"Select from iterable.\"\n",
    "    def __init__(self, idx=0):\n",
    "        super().__init__()\n",
    "        self.idx = idx\n",
    "    def forward(self, x):\n",
    "        return x[self.idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7680d7-c5d8-432e-bcc3-933cda787639",
   "metadata": {},
   "source": [
    "## Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de3fa7e-4d3c-4847-a721-92b16f9f5368",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from wafer.init import default_init, lambda_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb941ee-ac71-43ee-94f6-335b1fcb1be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SimpleRNN(nn.Module):\n",
    "    \"\"\"A RNN with its output mapped through a dense layer.\n",
    "\n",
    "    Input:  x, shape (N, L, D_in) or (N, D_in)\n",
    "    Output: outp, shape (N, L, D_out) or (N, D_out)\n",
    "    \"\"\"\n",
    "    def __init__(self, ni, nh, no, num_layers: int=2,\n",
    "                 actn: str='tanh',   # nonlinearity, ['tanh', 'relu']\n",
    "                 out_actn: Union[str, nn.Module]=None, # output activation\n",
    "                 init: str='normal', # initialization method ['uniform', 'normal', 'irnn', 'np-rnn']\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.recurrent = nn.RNN(ni, nh, num_layers=num_layers, batch_first=True, nonlinearity=actn)\n",
    "        self.dense = nn.Linear(nh, no)\n",
    "        self.out_actn = get_actn(out_actn)\n",
    "        # Initialize\n",
    "        self._init(self.recurrent, nh, init)\n",
    "        default_init(self.dense)\n",
    "\n",
    "    @staticmethod\n",
    "    @torch.no_grad()\n",
    "    def _init(m, nh, init):\n",
    "        for n,p in m.named_parameters():\n",
    "            if 'bias' in n:\n",
    "                nn.init.zeros_(p)\n",
    "            if 'weight_ih' in n:\n",
    "                if init == 'uniform':\n",
    "                    nn.init.uniform_(p, a=-1/np.sqrt(nh), b=1/np.sqrt(nh))\n",
    "                elif init == 'np-rnn':\n",
    "                    nn.init.normal_(p, std=1/np.sqrt(nh))\n",
    "                    p = p * (np.sqrt(2) * np.exp(1.2 / (max(nh, 6) - 2.4)))\n",
    "                    getattr(m, n).copy_(p)\n",
    "                else:\n",
    "                    nn.init.normal_(p, std=1/np.sqrt(nh))\n",
    "            if 'weight_hh' in n:\n",
    "                if init == 'uniform':\n",
    "                    nn.init.uniform_(p, a=-1/np.sqrt(nh), b=1/np.sqrt(nh))\n",
    "                elif init == 'irnn':\n",
    "                    nn.init.eye_(p)\n",
    "                elif init == 'np-rnn':\n",
    "                    nn.init.normal_(p)\n",
    "                    p = (p.T @ p) / nh\n",
    "                    p = p / np.linalg.eigvals(p.detach().numpy()).max()\n",
    "                    getattr(m, n).copy_(p)\n",
    "                else:\n",
    "                    nn.init.normal_(p, std=1/np.sqrt(nh))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        outp,_ = self.recurrent(x) # shape(N, L, nh)\n",
    "        return self.out_actn(self.dense(outp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbbdda8-9d1f-4bdf-81f8-11ffd8d53680",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LSTM(nn.Module):\n",
    "    \"\"\"Custom LSTM, allowing for different activations. Assuming `batch_first=True` and `bidirectional=False`.\n",
    "    \n",
    "    Inputs: x, h0_c0;\n",
    "        x: shape (N, L, D_in) or (N, D_in).\n",
    "        h0_c0: (h0, c0), Union[list, tuple], optional, default zeros, each of shape (num_layers, N, hidden_size).\n",
    "    \n",
    "    Outputs: output, (hn, cn);\n",
    "        output: shape (N, L, hidden_size) or (N, hidden_size), outputs of the last layer for each token.\n",
    "        hn: shape (num_layers, N, hidden_size), final hidden state.\n",
    "        cn: shape (num_layers, N, hidden_size), final cell state.\n",
    "    \"\"\"\n",
    "    def __init__(self, ni, nh, num_layers=1, actn: Union[str, nn.Module]='tanh', gate_actn: Union[str, nn.Module]='sigmoid',\n",
    "                 dropout=0.0, unit_forget_bias=True, init_gain=1/np.sqrt(3), recurrent_init_gain=1.):\n",
    "        super().__init__()\n",
    "        self.nh,self.num_layers = nh,num_layers\n",
    "        self.actn = get_actn(actn, nn.Tanh())\n",
    "        self.gate_actn = get_actn(gate_actn, nn.Sigmoid())\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        ws = []\n",
    "        for i in range(num_layers):\n",
    "            ws.append(nn.ModuleDict({\n",
    "                'ii': nn.Linear(ni if i == 0 else nh, nh),  # input-input weight\n",
    "                'if': nn.Linear(ni if i == 0 else nh, nh),  # input-forget weight\n",
    "                'io': nn.Linear(ni if i == 0 else nh, nh),  # input-output weight\n",
    "                'ic': nn.Linear(ni if i == 0 else nh, nh),  # input-cell weight\n",
    "                'hi': nn.Linear(nh, nh),                    # hidden-input weight\n",
    "                'hf': nn.Linear(nh, nh),                    # hidden-forget weight\n",
    "                'ho': nn.Linear(nh, nh),                    # hidden-output weight\n",
    "                'hc': nn.Linear(nh, nh),                    # hidden-cell weight\n",
    "            }))\n",
    "        self.ws = nn.ModuleList(ws)\n",
    "        \n",
    "        # Initialize\n",
    "        for n,p in self.named_parameters():\n",
    "            # hidden/recurrent weight\n",
    "            if '.h' in n:\n",
    "                if 'bias' in n: nn.init.zeros_(p)\n",
    "                else: nn.init.orthogonal_(p, recurrent_init_gain)\n",
    "            # non-recurrent weight\n",
    "            else:\n",
    "                if 'bias' in n:\n",
    "                    if '.if' in n and unit_forget_bias: nn.init.ones_(p)\n",
    "                    else: nn.init.zeros_(p)\n",
    "                else: nn.init.xavier_uniform_(p, init_gain) \n",
    "    \n",
    "    def _forward_single(self, x, h0_c0: list=None):\n",
    "        \"Forward pass of a single token.\"\n",
    "        if h0_c0 is None:\n",
    "            h0 = torch.zeros(self.num_layers, x.shape[0], self.nh, device=x.device)\n",
    "            c0 = torch.zeros(self.num_layers, x.shape[0], self.nh, device=x.device)\n",
    "        else:\n",
    "            h0,c0 = h0_c0\n",
    "            assert (h0.shape[-1] == c0.shape[-1] == self.nh) and (h0.shape[1] == c0.shape[1] == x.shape[0])\n",
    "        \n",
    "        hs, cs = [], []\n",
    "        for i in range(self.num_layers):\n",
    "            h, c = h0[i], c0[i]\n",
    "            i_gate = self.ws[i]['ii'](x) + self.ws[i]['hi'](h)\n",
    "            f_gate = self.ws[i]['if'](x) + self.ws[i]['hf'](h)\n",
    "            o_gate = self.ws[i]['io'](x) + self.ws[i]['ho'](h)\n",
    "            c_gate = self.ws[i]['ic'](x) + self.ws[i]['hc'](h)\n",
    "\n",
    "            i_gate = self.gate_actn(i_gate)\n",
    "            f_gate = self.gate_actn(f_gate)\n",
    "            o_gate = self.gate_actn(o_gate)\n",
    "            c_gate = self.actn(c_gate)\n",
    "\n",
    "            c_new = (f_gate * c) + (i_gate * c_gate)\n",
    "            h_new = o_gate * self.actn(c_new)\n",
    "            cs.append(c_new)\n",
    "            hs.append(h_new)\n",
    "            x = self.dropout(h_new)\n",
    "\n",
    "        hs, cs = torch.stack(hs, 0), torch.stack(cs, 0)\n",
    "        return (hs, cs)\n",
    "\n",
    "    def forward(self, x, h0_c0=None):\n",
    "        # Input `x`, shape (N, L, D) or (N, D); `h0_c0` = (h0, c0), each shape (num_layers, N, nh).\n",
    "        assert x.ndim in [2,3], f\"Expect 2D or 3D input, but got {x.ndim}D.\"\n",
    "        hs = []\n",
    "        cs = []\n",
    "        if x.ndim == 2: x = x.unsqueeze(1); reshape = True\n",
    "        else: reshape = False\n",
    "            \n",
    "        for xi in torch.permute(x, [1,0,2]):\n",
    "            hc = self._forward_single(xi, h0_c0)\n",
    "            hs.append(hc[0])\n",
    "            cs.append(hc[1])\n",
    "            h0_c0 = hc\n",
    "        output = torch.stack([h[-1] for h in hs], 1)\n",
    "        if reshape:\n",
    "            return output[:,0,:], (hs[-1], cs[-1])\n",
    "        return output, (hs[-1], cs[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f058e9e-de50-4852-bea0-e885f95eb1c8",
   "metadata": {},
   "source": [
    "## Input convex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a00c73-cbdb-4833-a963-de8dd8a3e868",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FICNN(nn.Module):\n",
    "    \"Fully input-convex NN. Refer to [ICNN](https://arxiv.org/abs/1609.07152).\"\n",
    "    def __init__(self, ni: int,                        # input size\n",
    "                 nh: Union[int, list[int]],            # hidden size\n",
    "                 no: int,                              # output size\n",
    "                 num_layer: int=2,                     # number of layers (include the output layer), if `nh` is a list, then the `num_layer = len(nh) + 1`\n",
    "                 actn: Union[str, nn.Module]='relu',   # hidden activation\n",
    "                 out_actn: Union[str, nn.Module]=None, # output activation\n",
    "                 init_gain: float=1.                   # weight initialization gain\n",
    "                ):\n",
    "        super().__init__()\n",
    "        nhs = [nh] * (num_layer - 1) if isinstance(nh, int) else nh\n",
    "        self.w_y = nn.ModuleList([nn.Linear(ni, nh) for nh in nhs + [no]])\n",
    "        self.w_z = nn.ModuleList([nn.Linear(i, j, bias=False) for i,j in zip(nhs, nhs[1:] + [no])])\n",
    "        self.actn = get_actn(actn, nn.ReLU())\n",
    "        self.out_actn = get_actn(out_actn)\n",
    "\n",
    "        lambda_init(self, lambda w,b: (nn.init.xavier_normal_(w, init_gain), nn.init.zeros_(b)))\n",
    "        self.weight_constraint()\n",
    "    \n",
    "    def weight_constraint(self):\n",
    "        \"Apply nonnegative weight constriant.\"\n",
    "        with torch.no_grad():\n",
    "            for n,p in self.w_z.named_parameters():\n",
    "                if 'weight' in n:\n",
    "                    p.clamp_min_(0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.w_y[0](x)\n",
    "        if len(self.w_z) == 0: return self.out_actn(z)\n",
    "        for wz,wy in zip(self.w_z[:-1], self.w_y[1:-1]):\n",
    "            z = self.actn(wz(z) + wy(x))\n",
    "        z = self.out_actn(self.w_z[-1](z) + self.w_y[-1](x))\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545f5b4a-7f52-4d72-a945-485f669c2d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ICRNN(nn.Module):\n",
    "    \"\"\"Input convex RNN. Refer to [Optimal Control Via Neural Networks: A Convex Approach](https://arxiv.org/abs/1805.11835).\n",
    "    \n",
    "    Inputs:  x, shape (N,L,D_in)\n",
    "    Outputs: outputs, shape (N,L,D_out)\n",
    "    \"\"\"\n",
    "    def __init__(self, ni: int,                        # input size\n",
    "                 nh: int,                              # hidden size\n",
    "                 no: int,                              # output size\n",
    "                 actn: Union[str, nn.Module]='relu',   # hidden activation\n",
    "                 out_actn: Union[str, nn.Module]=None, # output activation\n",
    "                 expand_inp: bool=True,                # expand the input to [x, -x]\n",
    "                 init_gain: float=1.                   # weight initialization gain\n",
    "                ):\n",
    "        super().__init__()\n",
    "        ni = ni * 2 if expand_inp else ni\n",
    "        self.expand_inp,self.nh = expand_inp,nh\n",
    "        self.actn = get_actn(actn, nn.ReLU())\n",
    "        self.out_actn = get_actn(out_actn)\n",
    "        self.U = nn.Linear(ni, nh)\n",
    "        self.V = nn.Linear(nh, no)\n",
    "        self.W = nn.Linear(nh, nh)\n",
    "        self.D1 = nn.Linear(nh, no)\n",
    "        self.D2 = nn.Linear(ni, nh)\n",
    "        self.D3 = nn.Linear(ni, no)\n",
    "\n",
    "        lambda_init(self, lambda w,b: (nn.init.xavier_normal_(w, init_gain), nn.init.zeros_(b)))\n",
    "        self.weight_constraint()\n",
    "\n",
    "    def weight_constraint(self):\n",
    "        \"Apply nonnegative weight constriant.\"\n",
    "        with torch.no_grad():\n",
    "            for n,p in self.named_parameters():\n",
    "                if 'weight' in n:\n",
    "                    p.clamp_min_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.expand_inp: x = torch.cat([x, -x], dim=-1)\n",
    "        x = torch.cat([torch.zeros(x.shape[0],1,x.shape[-1], device=x.device), x], dim=-2)\n",
    "        z_curr = z_prev = torch.zeros(x.shape[0], self.nh, device=x.device)\n",
    "        ys = []\n",
    "        for xi,xj in zip(x[:,:-1,:].permute(1,0,2), x[:,1:,:].permute(1,0,2)):\n",
    "            z_curr = self.actn(self.U(xj) + self.W(z_prev) + self.D2(xi))\n",
    "            ys.append(self.out_actn(self.V(z_curr) + self.D1(z_prev) + self.D3(xj)))\n",
    "            z_prev = z_curr\n",
    "        return torch.stack(ys, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffd4afb-c3d2-4233-a656-02c8f421edf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ICLSTMCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Modified LSTM for ICLSTM. From [ICLSTM](https://arxiv.org/abs/2311.07202).\n",
    "    Assuming `batch_first=True` and `bidirectional=False`.\n",
    "    \n",
    "    Inputs: x, h0_c0;\n",
    "        x: shape (N, L, D_in) or (N, D_in).\n",
    "        h0_c0: (h0, c0), Union[list, tuple], optional, default zeros, each of shape (num_layers, N, hidden_size).\n",
    "    \n",
    "    Outputs: output, (hn, cn);\n",
    "        output: shape (N, L, hidden_size) or (N, hidden_size), outputs of the last layer for each token.\n",
    "        hn: shape (num_layers, N, hidden_size), final hidden state.\n",
    "        cn: shape (num_layers, N, hidden_size), final cell state.\n",
    "    \"\"\"\n",
    "    def __init__(self, ni, nh, num_layers=1, actn: Union[str, nn.Module]='tanh', gate_actn: Union[str, nn.Module]='sigmoid',\n",
    "                 dropout=0.0, unit_forget_bias=True, init_gain=1/np.sqrt(3), recurrent_init_gain=1.):\n",
    "        super().__init__()\n",
    "        self.nh,self.num_layers = nh,num_layers\n",
    "        self.actn = get_actn(actn, nn.Tanh())\n",
    "        self.gate_actn = get_actn(gate_actn, nn.Sigmoid())\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        ws = []\n",
    "        ps = []\n",
    "        for i in range(num_layers):\n",
    "            ws.append(nn.ModuleDict({\n",
    "                'wi': nn.Linear(ni if i == 0 else nh, nh, bias=False),  # input base weight\n",
    "                'wh': nn.Linear(nh, nh, bias=False),                    # hidden base weight\n",
    "            }))\n",
    "            ps.append(nn.ParameterDict({\n",
    "                'bi': nn.Parameter(torch.empty(nh)),                    # input gate bias\n",
    "                'bf': nn.Parameter(torch.empty(nh)),                    # forget gate bias\n",
    "                'bo': nn.Parameter(torch.empty(nh)),                    # output gate bias\n",
    "                'bc': nn.Parameter(torch.empty(nh)),                    # cell gate bias\n",
    "                'sii': nn.Parameter(torch.empty(nh)),                   # input-input scaling\n",
    "                'sif': nn.Parameter(torch.empty(nh)),                   # input-forget scaling\n",
    "                'sio': nn.Parameter(torch.empty(nh)),                   # input-output scaling\n",
    "                'sic': nn.Parameter(torch.empty(nh)),                   # input-cell scaling\n",
    "                'shi': nn.Parameter(torch.empty(nh)),                   # hidden-input scaling\n",
    "                'shf': nn.Parameter(torch.empty(nh)),                   # hidden-forget scaling\n",
    "                'sho': nn.Parameter(torch.empty(nh)),                   # hidden-output scaling\n",
    "                'shc': nn.Parameter(torch.empty(nh))                    # hidden-cell scaling\n",
    "            }))\n",
    "        self.ws = nn.ModuleList(ws)\n",
    "        self.ps = nn.ParameterList(ps)\n",
    "        \n",
    "        # Initialize\n",
    "        for n,p in self.named_parameters():\n",
    "            # input base weight\n",
    "            if '.wi' in n: nn.init.xavier_uniform_(p, init_gain)\n",
    "            # hidden base weight\n",
    "            elif '.wh' in n: nn.init.orthogonal_(p, recurrent_init_gain)\n",
    "            else:\n",
    "                # bias\n",
    "                if '.b' in n:\n",
    "                    if '.bi' in n and unit_forget_bias: nn.init.ones_(p)\n",
    "                    else: nn.init.zeros_(p)\n",
    "                # scaling\n",
    "                else: nn.init.uniform_(p)\n",
    "    \n",
    "    def _forward_single(self, x, h0_c0: list=None):\n",
    "        \"Forward pass of a single token.\"\n",
    "        if h0_c0 is None:\n",
    "            h0 = torch.zeros(self.num_layers, x.shape[0], self.nh, device=x.device)\n",
    "            c0 = torch.zeros(self.num_layers, x.shape[0], self.nh, device=x.device)\n",
    "        else:\n",
    "            h0,c0 = h0_c0\n",
    "            assert (h0.shape[-1] == c0.shape[-1] == self.nh) and (h0.shape[1] == c0.shape[1] == x.shape[0])\n",
    "        \n",
    "        hs, cs = [], []\n",
    "        for i in range(self.num_layers):\n",
    "            h, c = h0[i], c0[i]\n",
    "            i_gate = self.ws[i]['wi'](x) * self.ps[i]['sii'] + self.ws[i]['wh'](h) * self.ps[i]['shi'] + self.ps[i]['bi']\n",
    "            f_gate = self.ws[i]['wi'](x) * self.ps[i]['sif'] + self.ws[i]['wh'](h) * self.ps[i]['shf'] + self.ps[i]['bf']\n",
    "            o_gate = self.ws[i]['wi'](x) * self.ps[i]['sio'] + self.ws[i]['wh'](h) * self.ps[i]['sho'] + self.ps[i]['bo']\n",
    "            c_gate = self.ws[i]['wi'](x) * self.ps[i]['sic'] + self.ws[i]['wh'](h) * self.ps[i]['shc'] + self.ps[i]['bc']\n",
    "\n",
    "            i_gate = self.gate_actn(i_gate)\n",
    "            f_gate = self.gate_actn(f_gate)\n",
    "            o_gate = self.gate_actn(o_gate)\n",
    "            c_gate = self.actn(c_gate)\n",
    "\n",
    "            c_new = (f_gate * c) + (i_gate * c_gate)\n",
    "            h_new = o_gate * self.actn(c_new)\n",
    "            cs.append(c_new)\n",
    "            hs.append(h_new)\n",
    "            x = self.dropout(h_new)\n",
    "\n",
    "        hs, cs = torch.stack(hs, 0), torch.stack(cs, 0)\n",
    "        return (hs, cs)\n",
    "\n",
    "    def forward(self, x, h0_c0=None):\n",
    "        # Input `x`, shape (N, L, D) or (N, D); `h0_c0` = (h0, c0), each shape (num_layers, N, nh).\n",
    "        assert x.ndim in [2,3], f\"Expect 2D or 3D input, but got {x.ndim}D.\"\n",
    "        hs = []\n",
    "        cs = []\n",
    "        if x.ndim == 2: x = x.unsqueeze(1); reshape = True\n",
    "        else: reshape = False\n",
    "            \n",
    "        for xi in torch.permute(x, [1,0,2]):\n",
    "            hc = self._forward_single(xi, h0_c0)\n",
    "            hs.append(hc[0])\n",
    "            cs.append(hc[1])\n",
    "            h0_c0 = hc\n",
    "        output = torch.stack([h[-1] for h in hs], 1)\n",
    "        if reshape:\n",
    "            return output[:,0,:], (hs[-1], cs[-1])\n",
    "        return output, (hs[-1], cs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0427aca-6ae0-478e-bfa2-a86c45827bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ICLSTM(nn.Module):\n",
    "    \"\"\"Input convex LSTM. From [ICLSTM](https://arxiv.org/abs/2311.07202).\n",
    "    \n",
    "    A L-ICLSTM is a stack of [ICLSTMCell, Linear, ReLU] repeated L times. The output is mapped through another Linear layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, ni, nh, no, num_layers=2,\n",
    "                 expand_inp=True, do_wi_nonneg=True, actn='relu', gate_actn='relu', out_actn=None, **kwargs):\n",
    "        super().__init__()\n",
    "        ni = ni * 2 if expand_inp else ni\n",
    "        self.expand_inp = expand_inp\n",
    "        self.actn = get_actn(actn, nn.ReLU())\n",
    "        self.out_actn = get_actn(out_actn)\n",
    "        self.blocks = nn.ModuleList([self._mk_block(ni, nh, actn, gate_actn, **kwargs) for _ in range(num_layers)])\n",
    "        self.dense_out = nn.Linear(ni, no)\n",
    "        default_init(self.dense_out)\n",
    "        # Nonnegative weights\n",
    "        self.do_wi_nonneg = do_wi_nonneg\n",
    "        self.weight_constraint()\n",
    "\n",
    "    def _mk_block(self, ni, nh, actn, gate_actn, **kwargs):\n",
    "        m = nn.Sequential(ICLSTMCell(ni, nh, 1, actn=actn, gate_actn=gate_actn, **kwargs),\n",
    "                          Select(),\n",
    "                          nn.Linear(nh, ni))\n",
    "        default_init(m[2], normal=False)\n",
    "        return m\n",
    "        \n",
    "    def weight_constraint(self):\n",
    "        \"Apply nonnegative weight constriant.\"\n",
    "        with torch.no_grad():\n",
    "            for n,p in self.named_parameters():\n",
    "                if '.wi' in n and not self.do_wi_nonneg: continue\n",
    "                if '.bias' in n: continue\n",
    "                p.clamp_min_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.expand_inp: x = torch.cat([x, -x], dim=-1)\n",
    "        x0 = x.clone()\n",
    "        for b in self.blocks:\n",
    "            x = self.actn(b(x)) + x0\n",
    "        return self.out_actn(self.dense_out(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dd444c-d8ce-4228-9df6-1bfe8b8fe459",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class WeightConstraintCB(Callback):\n",
    "    \"Weight constraint callback.\"\n",
    "    order = 10\n",
    "    def after_step(self): self.learner.model.weight_constraint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc805ec-3823-45ec-9d50-46aeae0dfb88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962e4275-a064-49cf-8742-11d8dfbb2bed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0042fa7-dbe1-41c6-83ed-56260a420f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd9b96c-7a2b-4b4a-8570-50a8701b97b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
