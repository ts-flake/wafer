{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b229f58-0c86-4a1b-a478-9950a36d2c10",
   "metadata": {},
   "source": [
    "# Core\n",
    "> Mimicking [fastai](https://docs.fast.ai) with minimal functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a9a575-5995-4577-8099-f7a7fb2580d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f24e075-5208-431c-85f5-11b8f1a6ecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6ed968-ae50-47ce-8ae8-32ab6895dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from wafer.imports import *\n",
    "from collections.abc import Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca8fa1d-c96f-4384-8845-92eda71bf669",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8565cb02-5a56-4d55-b4cc-f123b93d9582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def to_device(xs, device):\n",
    "    if isinstance(xs, (tuple, list)): return type(xs)(x.to(device=device) for x in list(xs))\n",
    "    if isinstance(xs, Mapping): return {k:v.to(device=device) for k,v in xs.items()}\n",
    "    return xs.to(device=device)\n",
    "\n",
    "def to_detach(xs):\n",
    "    if isinstance(xs, (tuple, list)): return type(xs)(x.detach() for x in list(xs))\n",
    "    if isinstance(xs, Mapping): return {k:v.detach() for k,v in xs.items()}\n",
    "    return xs.detach()\n",
    "\n",
    "def to_cpu(xs):\n",
    "    if isinstance(xs, (tuple, list)): return type(xs)(x.cpu() for x in list(xs))\n",
    "    if isinstance(xs, Mapping): return {k:v.cpu() for k,v in xs.items()}\n",
    "    return xs.cpu()\n",
    "\n",
    "def get_device(device=None):\n",
    "    device = 'cuda' if torch.cuda.is_available() else (device or 'cpu')\n",
    "    print(f\"Using {device} device.\")\n",
    "    return torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0583c5aa-b88f-4623-89aa-5f97c329dc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def has_children(m):\n",
    "    try: next(m.children())\n",
    "    except StopIteration: return False\n",
    "    return True\n",
    "\n",
    "def has_params(m): return len(list(m.parameters())) > 0\n",
    "\n",
    "def count_params(m):\n",
    "    \"Count trainable parameters.\"\n",
    "    return sum(p.numel() for p in m.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc585db4-9343-489f-a18a-54644f2ed54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_actn(actn: Union[str, nn.Module], default=Noop()):\n",
    "    \"Get the correct activation.\"\n",
    "    if isinstance(actn, str): return getattr(F, actn)\n",
    "    elif isinstance(actn, nn.Module): return actn\n",
    "    else: return default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bb94ae-b4e4-402d-922d-a5f5bfb69e3b",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bdce53-1c42-4fbf-a199-9c90c1de531c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Callback(): order = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ff840c-39f0-42c6-afc0-a3bb5c079562",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DeviceCB(Callback):\n",
    "    \"Move the model and batch to the correct device for training.\"\n",
    "    order = -10\n",
    "    def before_fit(self): self.learner.model.to(device=self.learner.device)\n",
    "    def before_batch(self): self.learner.batch = to_device(self.learner.batch, self.learner.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e9f4bd-1fdf-4745-a3ae-4afaf163e20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BatchXfmCB(Callback):\n",
    "    \"Apply a data transform to a batch.\"\n",
    "    order = -5\n",
    "    def __init__(self, xfm): self.xfm = xfm\n",
    "    def before_batch(self): self.learner.batch = self.xfm(self.learner.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5806bba8-56c2-4b6f-afbe-33339dde77e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MetricCB(Callback):\n",
    "    \"Using metrics from [`torcheval`](https://pytorch.org/torcheval/stable/) as callbacks.\"\n",
    "    order = 10\n",
    "    def __init__(self, metrics: Union[tuple, list],\n",
    "                 names: Union[str, tuple[str], list[str]]=None,\n",
    "                 train: bool=False # Compute metrics on train set; default on test set\n",
    "                ):\n",
    "        if not isinstance(metrics, (tuple, list)): metrics = [metrics]\n",
    "        if isinstance(names, str): names = [names]\n",
    "        if names is not None: assert len(metrics) == len(names), \"sizes of `names` and `metrics` do not match.\"\n",
    "        else: names = [type(metric).__name__ for metric in metrics]\n",
    "        self.metrics,self.names,self.train = metrics,names,train\n",
    "        for metric,name in zip(self.metrics, self.names): metric.name = name\n",
    "    \n",
    "    def reset(self):\n",
    "        for metric in self.metrics: metric.reset()\n",
    "    \n",
    "    @property\n",
    "    def log_df(self):\n",
    "        return pd.DataFrame(columns=self.names, data=self._log)\n",
    "\n",
    "    def before_fit(self):\n",
    "        self.reset()\n",
    "        self._log = []\n",
    "        self.learner.metrics = self\n",
    "\n",
    "    def before_epoch(self): self.reset()\n",
    "\n",
    "    def before_loss(self):\n",
    "        if (self.train and self.learner.training) or (not self.train and not self.learner.training):\n",
    "            for metric in self.metrics:\n",
    "                metric.update(self.learner.preds.detach().clone().cpu(), self.learner.yb.clone().cpu())\n",
    "    \n",
    "    def after_epoch(self):\n",
    "        self._log.append([metric.compute().item() for metric in self.metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08bc0f3-a43e-4710-813d-755fa3b500fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ProgressCB(Callback):\n",
    "    \"Log and display training infos.\"\n",
    "    order = MetricCB.order + 1\n",
    "\n",
    "    def before_fit(self):\n",
    "        self.learner.progress = self\n",
    "        self._has_no_log = False\n",
    "        cols = (  (['train_loss'] if self.learner.dls[0] != [] else [])\n",
    "                + (['test_loss'] if self.learner.dls[1] != [] else [])\n",
    "                + (self.learner.metrics.names if hasattr(self.learner, 'metrics') else [])\n",
    "                + (self.learner.extra_log.all_names if hasattr(self.learner, 'extra_log') else [])\n",
    "               )\n",
    "        if len(cols) == 0: self._has_no_log = True; return\n",
    "        self._log = pd.DataFrame(columns=cols)\n",
    "        self.disp_log = pd.DataFrame(columns=cols)\n",
    "        self.disp_log_id = None\n",
    "    \n",
    "    def before_epoch(self):\n",
    "        if self._has_no_log: return\n",
    "        if self.disp_log_id is None: self.disp_log_id = display(self.disp_log, display_id=True)\n",
    "        self._one_epoch = [[],[]]\n",
    "\n",
    "    def before_backward(self):\n",
    "        if self._has_no_log: return\n",
    "        self._one_epoch[0].append(self.learner.loss.item()) # batch train_loss\n",
    "\n",
    "    def after_loss(self):\n",
    "        if self._has_no_log: return\n",
    "        self._one_epoch[1].append(self.learner.loss.item()) # batch test_loss\n",
    "\n",
    "    def after_epoch(self):\n",
    "        if self._has_no_log: return\n",
    "        self._log.loc[self.learner.epoch] = ([np.mean(o) for o in self._one_epoch if o != []]\n",
    "                                             + (self.learner.metrics._log[-1] if hasattr(self.learner, 'metrics') else [])\n",
    "                                             + (self.learner.extra_log._data if hasattr(self.learner, 'extra_log') else [])\n",
    "                                            )\n",
    "        if self.learner.epoch % self.learner.disp_every == 0:\n",
    "            self.disp_log.loc[self.learner.epoch] = self._log.loc[self.learner.epoch]\n",
    "            self.disp_log_id.update(self.disp_log)\n",
    "        \n",
    "    def after_fit(self):\n",
    "        if self._has_no_log: return\n",
    "        self.learner.log = self._log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f672ef7-0734-4773-888d-5867f02f4cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseLogCB(Callback):\n",
    "    \"Base class. Log extra (other than standard train/test loss and metrics) infos.\"\n",
    "    order = ProgressCB.order-1\n",
    "\n",
    "    def __init__(self, names: list[list],  # Names of logged entries for each `func`\n",
    "                 funcs: list[callable],    # Funcstions to get the logged entries; each `f()` should take a single input `learner` and outputs a list/tuple of values\n",
    "                 keep: bool=False          # Keep a local log; logs can be found in `self.learner.log`\n",
    "                ):\n",
    "        self.names,self.funcs,self.keep = names,funcs,keep\n",
    "        self._data = []\n",
    "    \n",
    "    @property\n",
    "    def all_names(self):\n",
    "        return [oi for o in self.names for oi in o]\n",
    "    \n",
    "    @property\n",
    "    def log_df(self):\n",
    "        if not self.keep: print('No log found.'); return\n",
    "        return pd.DataFrame(columns=self.all_names, data=self._log)\n",
    "\n",
    "    def before_fit(self):\n",
    "        self.learner.extra_log = self\n",
    "        if self.keep: self._log = []\n",
    "\n",
    "    def before_epoch(self): self._data.clear()\n",
    "    def after_epoch(self):\n",
    "        for f in self.funcs:\n",
    "            val = f(self.learner)\n",
    "            if isinstance(val, (tuple, list)): self._data.extend(val)\n",
    "            elif isinstance(val, (int, float, bool, str)): self._data.append(val)\n",
    "            else: raise TypeError('Log function output should be tuple/list/scalar-like/string.')\n",
    "        if self.keep: self._log.append(self._data.copy())\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44123112-3e86-4215-991a-b6d742a8fa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ClipGradCB(Callback):\n",
    "    \"Clip the gradient norm.\"\n",
    "    def __init__(self, max_norm=1, norm_type=2, error_if_nonfinite=True, **kwargs):\n",
    "        self._norm = lambda x: nn.utils.clip_grad_norm_(x, max_norm, norm_type, error_if_nonfinite, **kwargs)\n",
    "    def before_step(self): self._norm(self.learner.model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40fe99d-50f3-40b6-bacb-91ac52924a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LRCB(Callback):\n",
    "    \"Learning rate callback.\"\n",
    "    order = ProgressCB.order + 1\n",
    "    def __init__(self, scheduler, on_batch=False):\n",
    "        self.scheduler,self.on_batch = scheduler,on_batch\n",
    "        \n",
    "    def after_step(self): \n",
    "        if self.on_batch:  self.scheduler.step()\n",
    "            \n",
    "    def after_epoch(self): \n",
    "        if not self.on_batch: self.scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a18adc9-f1da-4d8f-8614-9edfcaea190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ReduceLROnPlateauCB(LRCB):\n",
    "    \"ReduceLROnPlateau callback.\"\n",
    "    def __init__(self, scheduler, metric='test_loss'):\n",
    "        super().__init__(scheduler, on_batch=False)\n",
    "        self.metric = metric\n",
    "\n",
    "    def before_fit(self):\n",
    "        try:\n",
    "            self.learner.progress._log[self.metric]\n",
    "        except:\n",
    "            raise KeyError(f\"'{self.metric}' not found.\")\n",
    "\n",
    "    def after_step(self): pass\n",
    "            \n",
    "    def after_epoch(self):\n",
    "        val = self.learner.progress._log[self.metric].iloc[-1]\n",
    "        self.scheduler.step(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00e5042-707b-4fc0-8000-97e99cb7065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class EarlyStoppingCB(Callback):\n",
    "    \"Refer to `fastai`.\"\n",
    "    order = ProgressCB.order + 1\n",
    "\n",
    "    def __init__(self, monitor: str='test_loss', # name of the value being monitored\n",
    "                 comp = np.less,                 # numpy comparison operator, e.g. np.less or np.greater\n",
    "                 min_delta: float=0.,            # minimum distance between the latest value and the best monitored value\n",
    "                 patience: int=10,               # number of epoches to wait when no improvement of the model\n",
    "                 init_val = np.inf,              # initial best value\n",
    "                 reset_best: bool=True,          # reset best at each fit\n",
    "                ):\n",
    "        self.monitor,self.comp = monitor,comp\n",
    "        self.min_delta,self.patience = min_delta,patience\n",
    "        self._best = self.init_val = init_val\n",
    "        self.reset_best = reset_best\n",
    "\n",
    "    def before_fit(self):\n",
    "        try:\n",
    "            self.learner.progress._log[self.monitor]\n",
    "        except:\n",
    "            raise KeyError(f\"'{self.monitor}' to be monitored not found.\")\n",
    "        if self.reset_best: self._best = self.init_val\n",
    "        self._wait = 0\n",
    "        \n",
    "    def after_epoch(self):\n",
    "        val = self.learner.progress._log.get(self.monitor).iloc[-1]\n",
    "        if self.comp(val, self._best - self.min_delta): self._best,self._wait = val,0\n",
    "        else: self._wait += 1\n",
    "        \n",
    "        if self._wait >= self.patience:\n",
    "            print(f'No improvement since epoch {self.learner.epoch-self._wait}: early stopping')\n",
    "            raise CancelEpochException()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b62761a-a601-4836-a9c7-3b2e12872ca9",
   "metadata": {},
   "source": [
    "## Hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0486b1f6-f191-4031-b17a-150b2b72bb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Hook():\n",
    "    \"From `fastai`. Register a hook to `m` with `func`.\"\n",
    "    def __init__(self, m, func, forward=True, detach=True, cpu=True):\n",
    "        register = m.register_forward_hook if forward else m.register_full_backward_hook\n",
    "        self.hook,self.func = register(self.hook_func),func\n",
    "        self.stored,self.removed = None,False\n",
    "        self.detach,self.cpu = detach,cpu\n",
    "\n",
    "    def hook_func(self, m, i, o):\n",
    "        if self.detach: (i,o) = to_detach(i),to_detach(o)\n",
    "        if self.cpu: (i,o) = to_cpu(i),to_cpu(o)\n",
    "        self.stored = self.func(m, i, o)\n",
    "\n",
    "    def remove(self):\n",
    "        if not self.removed:\n",
    "            self.hook.remove()\n",
    "            self.removed = True\n",
    "    # context manager\n",
    "    def __enter__(self, *args): return self\n",
    "    def __exit__(self, *args): self.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f145c92-ff8d-4751-94b4-d8d878aef851",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Hooks():\n",
    "    \"From `fastai`. Register `Hook` to models in `ms`.\"\n",
    "    def __init__(self, ms, func, forward=True, detach=True, cpu=True):\n",
    "        self.hooks = [Hook(m, func, forward, detach, cpu) for m in ms]\n",
    "\n",
    "    def __getitem__(self,i): return self.hooks[i]\n",
    "    def __len__(self):       return len(self.hooks)\n",
    "    def __iter__(self):      return iter(self.hooks)\n",
    "    @property\n",
    "    def stored(self):        return [o.stored for o in self]\n",
    "\n",
    "    def remove(self):\n",
    "        for h in self.hooks: h.remove()\n",
    "\n",
    "    def __enter__(self, *args): return self\n",
    "    def __exit__ (self, *args): self.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12565e7d-d7c9-47e8-9af2-2aa8033dcdc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54c107c1-6ed2-4f9e-a8e7-2a44d0c60a96",
   "metadata": {},
   "source": [
    "## Learner\n",
    "A list of events:\n",
    "\n",
    "- `before_fit`: before fitting.\n",
    "- `before_epoch`: before every epoch.\n",
    "- `before_train`: before `model.train()`.\n",
    "- `before_batch`: *just* after a batch is drawn from the current dataloader.\n",
    "- `before_pred`: after unpacking a batch into `xb,yb`, before making predictions on `xb`.\n",
    "- `before_loss`: before calculating the loss, after making `preds`.\n",
    "- `before_backward`: before `optimizer.zero_grad()` and `loss.backward()`.\n",
    "- `before_step`: before `optimizer.step()`.\n",
    "- `after_step`: after `optimizer.step()`.\n",
    "- `after_batch`: after one batch is done.\n",
    "- `before_test`: before `model.eval()`.\n",
    "- `after_loss`: after the loss is computed, *only* in the *testing* loop.\n",
    "- `after_epoch`: after one epoch is done.\n",
    "- `after_fit`: after fitting.\n",
    "\n",
    "Attributes accessible to `Callback` via `self.learner` (after adding to a `Learner`)\n",
    "\n",
    "- `model`: the current model.\n",
    "- `dls`: the dataloaders (for training and testing).\n",
    "- `opt`: the optimizer.\n",
    "- `batch`: the current batch.\n",
    "- `xb`,`yb`: the current batched inputs and targets.\n",
    "- `preds`: the predictions on the current `xb`.\n",
    "- `loss`: the current loss (train/test).\n",
    "- `n_epochs`: the total number of epochs.\n",
    "- `epoch`: the current epoch.\n",
    "- `n_iters`: the current iterations from the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb49177a-67a2-44d1-9a73-14f38832fb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CancelEpochException(Exception): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d289cff-c49b-44f6-b79f-7d062f9d8535",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Learner():\n",
    "    \"One place to train/test a model.\"\n",
    "    _default_cbs = [DeviceCB(), ProgressCB()]\n",
    "    def __init__(self, model: nn.Module,  # The model to be trained\n",
    "                 dls: Union[tuple, list], # The dataloaders used for training and testing\n",
    "                 opt,                     # The optimizer used to update model's parameters\n",
    "                 loss_func,               # The loss function\n",
    "                 cbs=[],                  # Callbacks called in `order`\n",
    "                 disp_every: int=1,       # Display log every `disp_every` epochs\n",
    "                 device=torch.device('cpu')\n",
    "                ):\n",
    "        self.model,self.dls,self.opt,self.device = model,dls,opt,device\n",
    "        self.disp_every = disp_every\n",
    "        self.loss_func = loss_func\n",
    "        self.cbs = sorted(list(cbs) + self._default_cbs, key=lambda o: o.order) # sort callbacks according to their `order`\n",
    "        for cb in self.cbs: cb.learner  = self\n",
    "    @property\n",
    "    def training(self):\n",
    "        return self.model.training\n",
    "\n",
    "    def do_one_batch(self, train):\n",
    "        self('before_batch')\n",
    "        self.xb,self.yb = self.batch\n",
    "        self('before_pred')\n",
    "        self.preds = self.model(self.xb)\n",
    "        self('before_loss')\n",
    "        self.loss = self.loss_func(self.preds, self.yb)\n",
    "        if train:\n",
    "            self('before_backward')\n",
    "            self.opt.zero_grad()\n",
    "            self.loss.backward()\n",
    "            self('before_step')\n",
    "            self.opt.step()\n",
    "            self('after_step')\n",
    "        else:\n",
    "            self('after_loss')\n",
    "        self('after_batch')\n",
    "\n",
    "    def do_one_epoch(self):\n",
    "        self('before_epoch')\n",
    "        ### training ###\n",
    "        self('before_train')\n",
    "        # for self.batch in tqdm(self.dls[0], desc='Train', leave=False):\n",
    "        for self.batch in self.dls[0]:\n",
    "            self.model.train()\n",
    "            self.do_one_batch(True)\n",
    "            self.n_iters += 1\n",
    "        ### testing ###\n",
    "        self('before_test')\n",
    "        # for self.batch in tqdm(self.dls[1], desc='Test', leave=False):\n",
    "        for self.batch in self.dls[1]:\n",
    "            self.model.eval()\n",
    "            with torch.inference_mode():\n",
    "                self.do_one_batch(False)\n",
    "        self('after_epoch')\n",
    "\n",
    "    def fit(self, n_epochs: int=1):\n",
    "        self('before_fit')\n",
    "        self.n_epochs = n_epochs\n",
    "        self.n_iters = 0\n",
    "        for self.epoch in tqdm(range(n_epochs), desc='Epochs'):\n",
    "            try: self.do_one_epoch()\n",
    "            except CancelEpochException: break\n",
    "        self('after_fit')\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"Predict on an input instance.\"\n",
    "        self.model.eval()\n",
    "        x = to_device(x, self.device)\n",
    "        with torch.inference_mode():\n",
    "            try:\n",
    "                pred = self.model(x)\n",
    "            except:\n",
    "                pred = self.model(x.unsqueeze(0))\n",
    "            return pred\n",
    "\n",
    "    def predict_batch(self, xb=None):\n",
    "        \"Predict on a batch.\"\n",
    "        if xb is None:\n",
    "            xb = self.dls[0].one_batch()[0] if self.dls[1] == [] else self.dls[1].one_batch()[0]\n",
    "        xb = to_device(xb, self.device)\n",
    "        self.model.eval()\n",
    "        with torch.inference_mode():\n",
    "            preds = self.model(xb)\n",
    "        return preds\n",
    "\n",
    "    def plot_loss(self, ax=None, figsize=(3,3), title=\"\", logscale=False, skip: int =0):\n",
    "        if not hasattr(self, 'log'): return\n",
    "        assert isinstance(skip, int) and skip >= 0, 'skip must be an integer >= 0'\n",
    "        if ax is None: fig,ax = plt.subplots(figsize=figsize)\n",
    "        try: ax.plot(self.log['train_loss'].to_numpy()[skip:], c='r', label='train')\n",
    "        except: pass\n",
    "        try: ax.plot(self.log['test_loss'].to_numpy()[skip:], c='b', label='test')\n",
    "        except: pass\n",
    "        if logscale: ax.set_yscale('log')\n",
    "        ax.set_xlabel('epoch')\n",
    "        ax.set_ylabel('loss')\n",
    "        ax.set_title(title)\n",
    "        ax.legend(loc=1)\n",
    "        try: fig.tight_layout()\n",
    "        except: pass\n",
    "\n",
    "    def save(self, path:str, add_datetime: bool=True):\n",
    "        \"Save model's state dict and log.\"\n",
    "        torch.save(obj=self.model.state_dict(), f=path+'-model.pth')\n",
    "        if hasattr(self, 'log'):\n",
    "            self.log.to_csv(path+'-log.csv', index=False)\n",
    "            print('Model and log all saved.')\n",
    "        else:\n",
    "            print('Model saved.')\n",
    "        \n",
    "    def load(self, path: str):\n",
    "        \"Load model from saved state dict and log from csv.\"\n",
    "        if '-model.pth' in path: path = path.split('-model.pth')[0]\n",
    "        msg = self.model.load_state_dict(torch.load(path+'-model.pth'))\n",
    "        print(msg)\n",
    "        try:\n",
    "            self.log = pd.read_csv(path+'-log.csv')\n",
    "            print('Log loaded.')\n",
    "        except: pass\n",
    "        \n",
    "    def __call__(self, name):\n",
    "        for cb in self.cbs: getattr(cb, name, noop)()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35179654-8431-4472-acf4-b6e53f80a3d4",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d7a774-1ca8-483c-85eb-779673c89c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Dataloader(DataLoader):\n",
    "    \"Extension to `torch.utils.data.DataLoader`, to work with huggingface's `Dataset`.\"\n",
    "    def __init__(self, dataset, get_xy: callable, **kwargs):\n",
    "        super().__init__(dataset=dataset, **kwargs)\n",
    "        self.get_xy = get_xy\n",
    "    \n",
    "    def __iter__(self):\n",
    "        it = super().__iter__()\n",
    "        def _f():\n",
    "            for o in it:\n",
    "                yield tuple(self.get_xy(o))\n",
    "        return _f()\n",
    "\n",
    "    def one_batch(self):\n",
    "        return next(iter(self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8338a46e-aa96-48b4-964d-18c276eb9d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mk_dls_from_ds(ds,                        # Huggingface dataset\n",
    "                   get_xy: callable,          # A function to get (input, target) from a dict \n",
    "                   fields=['train', 'test'],  # Dict keys to split the dataset\n",
    "                   bs=[64, 64],               # Batch sizes\n",
    "                   shuffle=True,              # Shuffle the training set\n",
    "                  ):\n",
    "    \"Create train-test dataloaders.\"\n",
    "    if not isinstance(ds, (tuple, list)):\n",
    "        return (Dataloader(ds[fields[0]], get_xy, batch_size=bs[0], shuffle=shuffle),\n",
    "                Dataloader(ds[fields[1]], get_xy, batch_size=bs[1], shuffle=False))\n",
    "    else:\n",
    "        return (Dataloader(ds[0], get_xy, batch_size=bs[0], shuffle=shuffle),\n",
    "                Dataloader(ds[1], get_xy, batch_size=bs[1], shuffle=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4417253d-ebf3-4690-b932-5b0d27e4175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mk_dls_from_hub(name: str,                 # Name/path of the dataset\n",
    "                    get_xy: callable,          # A function to get (input, target) from a dict\n",
    "                    fields=['train', 'test'],  # Dict keys to split the dataset\n",
    "                    sz=[None, None],           # Sizes of train and test set, if `None` then returns all\n",
    "                    bs=[64, 64],               # Batch sizes\n",
    "                    shuffle=True,              # Shuffle the training set\n",
    "                    device=None\n",
    "                   ):\n",
    "    \"Conveience method to create train-test dataloaders from huggingface hub.\"\n",
    "    ds = load_dataset(name, device=device, trust_remote_code=True)\n",
    "    train = Dataset.from_dict(ds[fields[0]][:sz[0]]).with_format('torch') if sz[0] is not None else ds[fields[0]].with_format('torch')\n",
    "    test  = Dataset.from_dict(ds[fields[1]][:sz[1]]).with_format('torch') if sz[1] is not None else ds[fields[1]].with_format('torch')\n",
    "    return mk_dls_from_ds([train, test], get_xy, bs=bs, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edec18c-4420-4bb8-851e-95376a4355e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Scaler():\n",
    "    \"Simple wrapper of `sklearn.preprocessing`'s scalers.\"\n",
    "    def __init__(self, scaler, data: Union[list, np.ndarray],\n",
    "                 shaper = lambda o: np.reshape(o, (-1, np.shape(o)[-1])),  # function to reshape the `data` into (num_sample, num_feature)\n",
    "                 **kwargs):\n",
    "        self.shaper = shaper\n",
    "        self.scaler = scaler.fit(self.shaper(data), **kwargs)\n",
    "    \n",
    "    def xfm(self, data):\n",
    "        _shape = np.shape(data)\n",
    "        return self.scaler.transform(self.shaper(data)).reshape(_shape)\n",
    "    \n",
    "    def inv_xfm(self, data):\n",
    "        _shape = np.shape(data)\n",
    "        return self.scaler.inverse_transform(self.shaper(data)).reshape(_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d12295-b873-4da1-9d54-b76ae9dcaa13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86f9c4a-3bdb-4c7c-bd91-aea7373f9b17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3de9a96-d6ba-48c0-8df7-b22ad6b9a61a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294b724b-5ecb-43b8-a517-b673236346b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967489a7-09d7-4c20-aacb-273db482e3c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
