{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f860e55-e124-47b8-a681-3d992cc53cff",
   "metadata": {},
   "source": [
    "# Models\n",
    "> Ad hoc collection of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bd0976-185e-4466-b771-7f0d1d7b573d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602bac09-b12d-4bc6-bb41-71332a2794be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fce64b-38d7-43c0-9bb1-19cb1f3903cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from wafer.basics import *\n",
    "from wafer.init import default_init, lambda_init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7478d89-98d0-45ed-9138-5cb6be4f7195",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b63a139-b4f2-40d7-8e8d-83cf5f82df19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Lambda(nn.Module):\n",
    "    \"Turn a torch function into a Moudle.\"\n",
    "    def __init__(self, f):\n",
    "        super().__init__()\n",
    "        self.f = f\n",
    "    def forward(self, x): return self.f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699122ee-5075-4c73-bc9a-2dabcd1a2fd0",
   "metadata": {},
   "source": [
    "## basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e922329a-035d-449b-b589-40f71d364995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class XLinear(nn.Module):\n",
    "    \"KAN-like layer with RBF basis.\"\n",
    "    def __init__(self, ni, nb, no, fix_basis=False, actn='tanh'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.randn(1,no,ni,nb) / np.sqrt(no*ni))\n",
    "        self.actn = get_actn(actn)\n",
    "        # self.basis = nn.Conv1d(1,nb,1)\n",
    "        # self.centers = nn.Parameter(torch.linspace(-2,2,nb).reshape(1,1,-1), requires_grad=not fix_basis)\n",
    "        # self.widths = nn.Parameter(torch.ones(nb) * nb / np.sqrt(2) / 2, requires_grad=not fix_basis)\n",
    "        # self.basis_weight = nn.Parameter(torch.empty(nb))\n",
    "        # self.basis_bias   = nn.Parameter(torch.empty(nb))\n",
    "        self.basis_weight = nn.Parameter(torch.ones(nb).reshape(1,1,-1) * nb/np.sqrt(2)/2, requires_grad=not fix_basis)\n",
    "        self.basis_bias   = nn.Parameter(torch.linspace(-4,4,nb).reshape(1,1,-1), requires_grad=not fix_basis)\n",
    "        # default_init(self, normal=False)\n",
    "        # nn.init.uniform_(self.basis_weight, -3, 3)\n",
    "        # nn.init.uniform_(self.basis_bias, -3, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if x.ndim < 2: x = x.unsqueeze(0); flag = True\n",
    "        else: flag = False\n",
    "        x = x.unsqueeze(-1)\n",
    "        # X = self.actn(self.basis(x)).permute([0,2,1])\n",
    "        # X = torch.exp_(-(self.widths * (x - self.centers))**2)\n",
    "        X = self.actn(x * self.basis_weight + self.basis_bias)\n",
    "        X = torch.reshape(X, (X.shape[0], 1, *X.shape[1:]))\n",
    "        Y = self.weight * X\n",
    "        return torch.sum(Y, (-2,-1)) if not flag else torch.sum(Y, (-2,-1)).squeeze(0)\n",
    "    \n",
    "    def plot_basis(self, idx=0, xl=-2, xu=2):\n",
    "        fig,ax = plt.subplots(figsize=(3,3))\n",
    "        l = torch.linspace(xl, xu, 100)\n",
    "        x = torch.stack([l, l], dim=-1)\n",
    "        x = x.unsqueeze(-1)\n",
    "        # x = x.unsqueeze(1)\n",
    "        with torch.no_grad():\n",
    "            # X = torch.exp_(-(self.widths * (x - self.centers))**2)\n",
    "            # X = self.actn(self.basis(x)).permute([0,2,1])\n",
    "            X = self.actn(x * self.basis_weight + self.basis_bias)\n",
    "        for i in range(X.shape[-1]):\n",
    "            ax.plot(l, X[:,idx,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5c64fe-f744-4ead-bc87-cbe6442c149f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mm = XLinear(2,8,1,actn=Lambda(lambda o: torch.exp(-o*o)))\n",
    "mm = nn.Sequential(XLinear(2,8,5), XLinear(5,8,3))\n",
    "count_params(mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1e548f-ebc6-4d64-a861-efae08d14860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mm.plot_basis(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fb97e5-87d2-498c-a665-416038758d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%timeit\n",
    "xx = torch.randn(2)\n",
    "mm(xx).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c896c74a-db71-462b-bb86-d9b1af0170be",
   "metadata": {},
   "source": [
    "## Input convex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a00c73-cbdb-4833-a963-de8dd8a3e868",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FICNN(nn.Module):\n",
    "    \"Fully input-convex NN. Refer to [ICNN](https://arxiv.org/abs/1609.07152).\"\n",
    "    def __init__(self, ni: int,                        # input size\n",
    "                 nh: Union[int, list[int]],            # hidden size\n",
    "                 no: int,                              # output size\n",
    "                 num_layer: int=2,                     # number of layers (include the output layer), if `nh` is a list, then the `num_layer = len(nh) + 1`\n",
    "                 actn: Union[str, nn.Module]='relu',   # hidden activation\n",
    "                 out_actn: Union[str, nn.Module]=None, # output activation\n",
    "                 init_gain: float=1.                   # weight initialization gain\n",
    "                ):\n",
    "        super().__init__()\n",
    "        nhs = [nh] * (num_layer - 1) if isinstance(nh, int) else nh\n",
    "        self.w_y = nn.ModuleList([nn.Linear(ni, nh) for nh in nhs + [no]])\n",
    "        self.w_z = nn.ModuleList([nn.Linear(i, j, bias=False) for i,j in zip(nhs, nhs[1:] + [no])])\n",
    "        self.actn = get_actn(actn, nn.ReLU())\n",
    "        self.out_actn = get_actn(out_actn)\n",
    "\n",
    "        # Initialize\n",
    "        lambda_init(self, lambda w,b: (nn.init.xavier_normal_(w, init_gain), nn.init.zeros_(b)))\n",
    "        self.weight_constraint()\n",
    "    \n",
    "    def weight_constraint(self):\n",
    "        \"Apply nonnegative weight constriant.\"\n",
    "        with torch.no_grad():\n",
    "            for n,p in self.w_z.named_parameters():\n",
    "                if 'weight' in n:\n",
    "                    p.clamp_min_(0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.w_y[0](x)\n",
    "        if len(self.w_z) == 0: return self.out_actn(z)\n",
    "        for wz,wy in zip(self.w_z[:-1], self.w_y[1:-1]):\n",
    "            z = self.actn(wz(z) + wy(x))\n",
    "        z = self.out_actn(self.w_z[-1](z) + self.w_y[-1](x))\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5118bc-ee83-4246-b6c7-bdd5a89d33a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728d1410-bd46-4f99-9014-a682c3afd6d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd9b96c-7a2b-4b4a-8570-50a8701b97b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec386e97-dfbc-4d36-aca2-57a9ec1c62ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
