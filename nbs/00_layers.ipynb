{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers\n",
    "\n",
    "> To build and initialize NN layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore.all import *\n",
    "from fastai.torch_basics import *\n",
    "from fastai.callback.hook import Hook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "_fc_conv_filter = (nn.Linear, nn.Conv1d, nn.Conv2d, nn.Conv3d,\n",
    "                   nn.ConvTranspose1d, nn.ConvTranspose2d, nn.ConvTranspose3d)\n",
    "\n",
    "def lambda_init(m: nn.Module, func: callable=lambda w,b: (nn.init.kaiming_normal_(w), nn.init.zeros_(b))):\n",
    "    \"Initialize the `weight` and `bias` of a model `m` with `func`.\"\n",
    "    for l in m.modules():\n",
    "        if isinstance(l, _fc_conv_filter):\n",
    "            if l.bias is None: func(l.weight, torch.empty(1))\n",
    "            else:              func(l.weight, l.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def lsuv_init(m: nn.Module, # model\n",
    "              xb: torch.Tensor, # mini-batch input\n",
    "              tol: float=0.01, # tolerance\n",
    "              n_iter: int=10, # maximum number of iterations\n",
    "              verbose: bool=False, # print out details\n",
    "             ):\n",
    "    \"Refer to [All you need is a good init](https://arxiv.org/abs/1511.06422).\"\n",
    "    xb = xb.cpu()\n",
    "    m.to(xb.device)\n",
    "\n",
    "    # orthogonal init\n",
    "    lambda_init(m, lambda w,b: (nn.init.orthogonal_(w), nn.init.zeros_(b)))\n",
    "\n",
    "    # LSUV init\n",
    "    m.eval()\n",
    "    for l in m.modules():\n",
    "        if isinstance(l, _fc_conv_filter):\n",
    "            n = 0\n",
    "            h = Hook(l, lambda m,i,o: (o.mean().item(), o.std().item()), cpu=True)\n",
    "            with torch.inference_mode():\n",
    "                while (m(xb) is not None\n",
    "                       and ((l.bias is not None and abs(h.stored[0] - 0.) > tol) or abs(h.stored[1] - 1.) > tol)\n",
    "                       and n < n_iter):\n",
    "                    l.weight /= (h.stored[1] + 1e-8)\n",
    "                    if l.bias is not None: l.bias -= h.stored[0]\n",
    "                    n += 1\n",
    "            if verbose: print(f\"{str(l):80}| took {n} iterations, mean={h.stored[0]:7.4f}, std={h.stored[1]:.4f}\")\n",
    "            h.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `xb` mini-batch is used to estimate the statistics (mean and std) for scaling the weights, similar to `BatchNorm`, but only for initialization. The batch size of `xb` could be different to the actual size used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "tst_model = nn.Sequential(nn.Conv2d(3,10,3), nn.ReLU(),\n",
    "                          nn.Conv2d(10,10,1, bias=False), nn.LeakyReLU(),\n",
    "                          nn.Flatten(), nn.Linear(640,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1))                                | took 0 iterations, mean= 0.0023, std=0.9994\n",
      "Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)                   | took 1 iterations, mean=-0.1563, std=1.0000\n",
      "Linear(in_features=640, out_features=10, bias=True)                             | took 2 iterations, mean=-0.0000, std=1.0000\n"
     ]
    }
   ],
   "source": [
    "xb = torch.randn(50,3,10,10)\n",
    "lsuv_init(tst_model, xb, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastai.callback.hook import Hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def print_forward_stats(mod, xb):\n",
    "    _print_stats = lambda m,i,o: print(f'{type(m).__name__:12}| mean={o.mean():7.4f}, std={o.std():.4f}')\n",
    "    hooks = Hooks(mod, _print_stats)\n",
    "    with torch.inference_mode():\n",
    "        mod(xb)\n",
    "    hooks.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d      | mean=-0.0115, std=1.0142\n",
      "ReLU        | mean= 0.3987, std=0.5902\n",
      "Conv2d      | mean=-0.1626, std=1.0073\n",
      "LeakyReLU   | mean= 0.3056, std=0.5329\n",
      "Flatten     | mean= 0.3056, std=0.5329\n",
      "Linear      | mean=-0.0253, std=0.9501\n"
     ]
    }
   ],
   "source": [
    "print_forward_stats(tst_model, torch.randn(50,3,10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def default_init(m: nn.Module, # model\n",
    "                 normal: bool=True, # use normal distribution\n",
    "                 verbose: bool=False, # print out details\n",
    "                ):\n",
    "    \"Initialize weights of `nn.Linear` and `nn.ConvXd` using Xavier's or Kaiming's method; zero biases; custom gains.\"\n",
    "    if normal: xavier = nn.init.xavier_normal_;  kaiming = nn.init.kaiming_normal_\n",
    "    else:      xavier = nn.init.xavier_uniform_; kaiming = nn.init.kaiming_uniform_\n",
    "    _txt = 'normal' if normal else 'uniform'\n",
    "    \n",
    "    _actn_filter = (nn.Tanh, nn.Sigmoid, nn.Softplus, nn.Softsign,\n",
    "                    nn.ReLU, nn.LeakyReLU, nn.SiLU, nn.GELU, nn.ELU)\n",
    "    _m = list(filter(lambda o: isinstance(o, _fc_conv_filter + _actn_filter), m.modules()))\n",
    "    \n",
    "    for l,lm in zip(_m, _m[0:1]+_m[:-1]): # l-th and (l-1)-th layers\n",
    "        if isinstance(l, _fc_conv_filter):\n",
    "            # ReLU and its variants\n",
    "            if isinstance(lm, (nn.ReLU, nn.LeakyReLU, nn.SiLU, nn.GELU, nn.Softsign)):\n",
    "                gain = lm.negative_slope if isinstance(lm, nn.LeakyReLU) else 0.\n",
    "                kaiming(l.weight, gain, nonlinearity='leaky_relu')\n",
    "                if verbose: print(f\"{str(l):80}| He_{_txt}, negative_slope={gain}\")\n",
    "            \n",
    "            # Custom gains by trial and error\n",
    "            elif isinstance(lm, (nn.Tanh, nn.Sigmoid, nn.Softplus, nn.ELU)):\n",
    "                gain = 1.79 if isinstance(lm, nn.Sigmoid) else 1.17\n",
    "                xavier(l.weight, gain)\n",
    "                if verbose: print(f\"{str(l):80}| Xavier_{_txt}, gain={gain}\")\n",
    "            else:\n",
    "                xavier(l.weight)\n",
    "                if verbose: print(f\"{str(l):80}| Xavier_{_txt}, gain=1.\")\n",
    "            if l.bias is not None: nn.init.zeros_(l.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1))                                | Xavier_normal, gain=1.\n",
      "Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)                   | He_normal, negative_slope=0.0\n",
      "Linear(in_features=640, out_features=10, bias=True)                             | He_normal, negative_slope=0.01\n"
     ]
    }
   ],
   "source": [
    "default_init(tst_model,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d      | mean=-0.0086, std=0.6737\n",
      "ReLU        | mean= 0.2621, std=0.3903\n",
      "Conv2d      | mean=-0.0558, std=0.6318\n",
      "LeakyReLU   | mean= 0.2018, std=0.3607\n",
      "Flatten     | mean= 0.2018, std=0.3607\n",
      "Linear      | mean=-0.0345, std=0.5574\n"
     ]
    }
   ],
   "source": [
    "print_forward_stats(tst_model, torch.randn(50,3,10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def rai_init(m: nn.Module):\n",
    "    \"Randomized asymmetric initializer. \\\n",
    "    Refer to [Dying ReLU and Initialization: Theory and Numerical Examples](https://arxiv.org/abs/1903.06733)\"\n",
    "    _is_first = True\n",
    "    \n",
    "    for l in m.modules():\n",
    "        if isinstance(l, _fc_conv_filter):\n",
    "            if _is_first:\n",
    "                nn.init.kaiming_normal_(l.weight)\n",
    "                if l.bias is not None: nn.init.zeros_(l.bias)\n",
    "                _is_first = False; continue\n",
    "            # Get correct fan_in and fan_out\n",
    "            if l.weight.ndim == 2:\n",
    "                fan_out,fan_in = l.weight.shape\n",
    "            else:\n",
    "                fan_out = l.weight.shape[0]\n",
    "                fan_in = np.prod(l.weight.shape[1:])\n",
    "            # RAI\n",
    "            V = torch.randn(fan_out, fan_in+1) * 0.6007 / (fan_in ** 0.5)\n",
    "            for j in range(fan_out):\n",
    "                k = np.random.randint(0, fan_in+1)\n",
    "                V[j,k] = np.random.beta(2, 1)\n",
    "            with torch.no_grad():\n",
    "                l.weight.copy_(V[:, :-1].reshape(l.weight.shape))\n",
    "                if l.bias is not None: l.bias.copy_(V[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
